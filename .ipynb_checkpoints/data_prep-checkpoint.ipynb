{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# periodic time series\n",
    "\n",
    "#  time series modeling process\n",
    "\n",
    "# 1. stationary test\n",
    "#    trend model:\n",
    "#         Aggregation – taking average for a time period like monthly/weekly averages\n",
    "#         Smoothing – taking rolling averages\n",
    "#         Polynomial Fitting – fit a regression model\n",
    "#    seasonality model:\n",
    "#         differencing\n",
    "#         model fitting\n",
    "# 2. stationalize time series\n",
    "#       detrend\n",
    "#       remove seasonality\n",
    "# 3. model the stationary part of time series        \n",
    "\n",
    "#    seasonal-ARIMA \n",
    "\n",
    "# 4. perform prediction \n",
    "\n",
    "\n",
    "# TO DO\n",
    "# plain time series prediction\n",
    "# periodic time series prediction\n",
    "\n",
    "# https://dandelion.eu/datamine/open-big-data/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://www.johnwittenauer.net/a-simple-time-series-analysis-of-the-sp-500-index/\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO: \n",
    "\n",
    "# Statistics programming \n",
    "\n",
    "# book:\n",
    "\n",
    "# Probabilistic Programming and Bayesian Methods for Hackers \n",
    "# http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Prologue/Prologue.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Examples:\n",
    "\n",
    "# stock prediction\n",
    "# https://bitbucket.org/joexdobs/ml-classifier-gesture-recognition/wiki/stock-example/\n",
    "# predict-future-stock-price-using-machine-learning.md\n",
    "\n",
    "# statsmodels:\n",
    "#  http://statsmodels.sourceforge.net/stable/index.html\n",
    "\n",
    "# http://www.seanabu.com/2016/03/22/time-series-seasonal-ARIMA-model-in-python/\n",
    "\n",
    "# http://nbviewer.jupyter.org/gist/ChadFulton/5127108f4c7025ed2648\n",
    "\n",
    "# https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/\n",
    "\n",
    "# http://nbviewer.jupyter.org/github/jakevdp/SeattleBike/blob/master/SeattleCycling.ipynb\n",
    "\n",
    "# book\n",
    "# http://shelfjoy.com/shelfjoy/17-essential-machine-learning-books-suggested-by-michael-i-jordan-from-berkeley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guo/.local/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline    \n",
    "import matplotlib as mplt\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.style.use('ggplot')\n",
    "\n",
    "from utils_libs import *\n",
    "from utils_data_prep import *\n",
    "\n",
    "from scipy.stats import lognorm\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import chisqprob\n",
    "\n",
    "from numpy import prod\n",
    "import seaborn as sns\n",
    "\n",
    "# statiscal models\n",
    "import statsmodels as sm\n",
    "from statsmodels.tsa.stattools import acf  \n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from statsmodels.tsa.api import VAR, DynamicVAR\n",
    "\n",
    "from statsmodels.stats import diagnostic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539157 10641\n"
     ]
    }
   ],
   "source": [
    "# --- Load pre-processed order book data ---\n",
    "\n",
    "all_dta_minu = np.load(\"../dataset/bitcoin/dta_minu.dat\")\n",
    "all_loc_hour = np.load(\"../dataset/bitcoin/loc_hour.dat\")\n",
    "print len(all_dta_minu), len(all_loc_hour)\n",
    "\n",
    "# --- Load order book data files ---\n",
    "\n",
    "# all_dta_minu,all_loc_hour = load_raw_order_book_files('../dataset/bitcoin/order_book/*.csv', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(539157,) (539157, 2)\n",
      "539157 528516 10641 10641\n"
     ]
    }
   ],
   "source": [
    "# --- calculate price, return and volatility ---\n",
    "\n",
    "price_minu, req_minu = cal_price_req_minu(all_dta_minu)\n",
    "\n",
    "print np.shape(price_minu), np.shape(req_minu)\n",
    "\n",
    "pvol_hour = cal_price_volatility_hour( all_loc_hour, price_minu )\n",
    "return_minu, rvol_hour = cal_return_volatility_hour( all_loc_hour, price_minu, 'per' )\n",
    "\n",
    "print len(price_minu),len(return_minu), len(pvol_hour), len(rvol_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(539157, 10)\n"
     ]
    }
   ],
   "source": [
    "# --- extract features w.r.t. minute ---\n",
    "features_minu = [] \n",
    "# ask mean price, mean amount, var price, var amount, \n",
    "# bid mean price, mean amount, var price, var amount,\n",
    "# ask skew price, skew amount\n",
    "# bid skew price, skew amount, \n",
    "# ask request, bid request\n",
    "\n",
    "for i in range( len(all_dta_minu) ):\n",
    "    \n",
    "    # shape: # by [price, amount]\n",
    "    tmp_a = all_dta_minu[i][0]\n",
    "    tmp_b = all_dta_minu[i][1]\n",
    "\n",
    "    '''\n",
    "    tmpft = mle_norm_2d(tmp_a)\n",
    "    tmp = tmpft[0] + tmpft[1]\n",
    "    \n",
    "    tmpft = mle_norm_2d(tmp_b)\n",
    "    tmp += tmpft[0]\n",
    "    tmp += tmpft[1]\n",
    "    \n",
    "    # skewness feature\n",
    "    tmp += skewness(tmp_a)\n",
    "    tmp += skewness(tmp_b)\n",
    "    \n",
    "    # amount of requests\n",
    "    tmp += [len(all_dta_minu[i][0]), len(all_dta_minu[i][1])] \n",
    "    \n",
    "    '''\n",
    "    features_minu.append( orderbook_stat_features(all_dta_minu, i) )\n",
    "    \n",
    "#  shape: [miniutes, features]\n",
    "features_minu = np.reshape( features_minu, (len(features_minu), -1) )\n",
    "\n",
    "print np.shape( np.asarray(features_minu) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_testing_garch(features_minu, vol_hour, all_loc_hour, \\\n",
    "                                  order_minu, order_hour, train_split_ratio, price_minu):\n",
    "    tmpcnt = len(vol_hour)\n",
    "    tmp_split = int(train_split_ratio*(tmpcnt - order_hour - 1)) + order_hour\n",
    "    \n",
    "    minu_idx = all_loc_hour[tmp_split]\n",
    "    \n",
    "    price_train = price_minu[:minu_idx+1]\n",
    "    price_test  = price_minu[minu_idx+1:]\n",
    "    \n",
    "    return_train = []\n",
    "    for i in range(1, len(price_train)):\n",
    "        return_train.append( (price_train[i]-price_train[i-1])/(price_train[i-1]+1e-5)*100 )\n",
    "        \n",
    "    return_test = []\n",
    "    for i in range(1, len(price_test)):\n",
    "        return_test.append( (price_test[i]-price_test[i-1])/(price_test[i-1]+1e-5)*100 )\n",
    "            \n",
    "    # vol train, return train, vol test, return test\n",
    "    return vol_hour[1:tmp_split+1], return_train, vol_hour[tmp_split+1:], return_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8515,) (419602,) (2125,) (119553,)\n",
      "539155 10640\n"
     ]
    }
   ],
   "source": [
    "# --- obtain training and testing data, garch ---\n",
    "para_order_minu = 10\n",
    "para_order_hour = 16\n",
    "para_train_split_ratio = 0.8\n",
    "file_postfix = \"garch\" \n",
    "#mle_norm, map_norm, map_lognorm \n",
    "\n",
    "vol_train, rt_train, vol_test, rt_test = training_testing_garch(features_minu, rvol_hour, all_loc_hour, \\\n",
    "                                   para_order_minu, para_order_hour, para_train_split_ratio, price_minu)\n",
    "\n",
    "print np.shape(vol_train), np.shape(rt_train), np.shape(vol_test), np.shape(rt_test)\n",
    "print len(rt_train)+len(rt_test), len(vol_train)+len(vol_test)\n",
    "\n",
    "np.asarray(vol_train).dump(\"../dataset/bitcoin/training_data/voltrain_\"+file_postfix+\".dat\")\n",
    "np.asarray(rt_train).dump(\"../dataset/bitcoin/training_data/rttrain_\"  +file_postfix+\".dat\")\n",
    "np.asarray(vol_test).dump(\"../dataset/bitcoin/training_data/voltest_\"  +file_postfix+\".dat\")\n",
    "np.asarray(rt_test).dump(\"../dataset/bitcoin/training_data/rttest_\"    +file_postfix+\".dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8515,) (8515, 140) (2125,) (2125, 140)\n"
     ]
    }
   ],
   "source": [
    "# --- obtain training and testing data, arima, structural time series ---\n",
    "para_order_minu = 10\n",
    "para_order_hour = 16\n",
    "para_train_split_ratio = 0.8\n",
    "file_postfix = \"stat\" \n",
    "#mle_norm, map_norm, map_lognorm \n",
    "\n",
    "xtrain, extrain, xtest, extest = training_testing_statistic(features_minu, rvol_hour, all_loc_hour, \\\n",
    "                                   para_order_minu, para_order_hour, para_train_split_ratio)\n",
    "\n",
    "print np.shape(xtrain), np.shape(extrain), np.shape(xtest), np.shape(extest)\n",
    "\n",
    "np.asarray(xtrain).dump(\"../dataset/bitcoin/training_data/xtrain_\"+file_postfix+\".dat\")\n",
    "np.asarray(xtest ).dump(\"../dataset/bitcoin/training_data/xtest_\" +file_postfix+\".dat\")\n",
    "np.asarray(extrain).dump(\"../dataset/bitcoin/training_data/extrain_\"+file_postfix+\".dat\")\n",
    "np.asarray(extest ).dump(\"../dataset/bitcoin/training_data/extest_\" +file_postfix+\".dat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16,) (10, 4)\n",
      "(8500, 56) (8500,) (2125, 56) (2125,)\n"
     ]
    }
   ],
   "source": [
    "# --- obtain training and testing data, plain regression ---\n",
    "para_order_minu = 10\n",
    "para_order_hour = 16\n",
    "para_train_split_ratio = 0.8\n",
    "file_postfix = \"v_minu_reg\" \n",
    "\n",
    "bool_feature_selection = True\n",
    "\n",
    "#mle_norm, map_norm, map_lognorm \n",
    "\n",
    "# features_minu, req_minu, pvol_hour, all_loc_hour\n",
    "x, y, var_explain = prepare_feature_target( features_minu, [], rvol_hour, all_loc_hour, \\\n",
    "                                                        para_order_minu, para_order_hour, bool_feature_selection )\n",
    "print np.shape(x[0][0]), np.shape(x[0][1])\n",
    "    \n",
    "xtrain, ytrain, xtest, ytest = training_testing_plain_regression(x, y, para_train_split_ratio)    \n",
    "print np.shape(xtrain), np.shape(ytrain), np.shape(xtest), np.shape(ytest)\n",
    "\n",
    "# np.asarray(xtrain).dump(\"../dataset/bitcoin/training_data/xtrain_\"+file_postfix+\".dat\")\n",
    "# np.asarray(xtest ).dump(\"../dataset/bitcoin/training_data/xtest_\" +file_postfix+\".dat\")\n",
    "# np.asarray(ytrain).dump(\"../dataset/bitcoin/training_data/ytrain_\"+file_postfix+\".dat\")\n",
    "# np.asarray(ytest ).dump(\"../dataset/bitcoin/training_data/ytest_\" +file_postfix+\".dat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10625\n",
      "2 (8500,) (2125,)\n"
     ]
    }
   ],
   "source": [
    "# --- obtain training and testing data, mixture: linear, mlp --- \n",
    "para_order_minu = 10\n",
    "para_order_hour = 16\n",
    "para_train_split_ratio = 0.8 \n",
    "file_postfix = \"v_minu_mix\"\n",
    "\n",
    "# features_minu, req_minu, pvol_hour, all_loc_hour\n",
    "x, y = prepare_feature_target( features_minu, [], rvol_hour, all_loc_hour, \\\n",
    "                                                        para_order_minu, para_order_hour )\n",
    "print len(x)\n",
    "\n",
    "xtrain, ytrain, xtest, ytest = training_testing_mixture_mlp(x, y, para_train_split_ratio)\n",
    "\n",
    "print len(xtrain[0]), np.shape(ytrain), np.shape(ytest)\n",
    "\n",
    "np.asarray(xtrain).dump(\"../dataset/bitcoin/training_data/xtrain_\"+file_postfix+\".dat\")\n",
    "np.asarray(xtest ).dump(\"../dataset/bitcoin/training_data/xtest_\" +file_postfix+\".dat\")\n",
    "np.asarray(ytrain).dump(\"../dataset/bitcoin/training_data/ytrain_\"+file_postfix+\".dat\")\n",
    "np.asarray(ytest ).dump(\"../dataset/bitcoin/training_data/ytest_\" +file_postfix+\".dat\")\n",
    "\n",
    "#  done for training-test preparaion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10577\n",
      "2 (8461,) (2116,)\n"
     ]
    }
   ],
   "source": [
    "# --- obtain training and testing data, mixture: rnn --- \n",
    "para_order_minu = 10\n",
    "para_order_hour = 64\n",
    "para_train_split_ratio = 0.8\n",
    "file_postfix = \"v_minu_mix_rnn\"\n",
    "\n",
    "# features_minu, req_minu, pvol_hour, all_loc_hour\n",
    "x, y = prepare_feature_target( features_minu, [], pvol_hour, all_loc_hour, \\\n",
    "                                                        para_order_minu, para_order_hour )\n",
    "print len(x)\n",
    "\n",
    "xtrain, ytrain, xtest, ytest = training_testing_mixture_rnn(x, y, para_train_split_ratio)\n",
    "\n",
    "print len(xtrain[0]), np.shape(ytrain), np.shape(ytest)\n",
    "\n",
    "np.asarray(xtrain).dump(\"../dataset/bitcoin/training_data/xtrain_\"+file_postfix+\".dat\")\n",
    "np.asarray(xtest ).dump(\"../dataset/bitcoin/training_data/xtest_\" +file_postfix+\".dat\")\n",
    "np.asarray(ytrain).dump(\"../dataset/bitcoin/training_data/ytrain_\"+file_postfix+\".dat\")\n",
    "np.asarray(ytest ).dump(\"../dataset/bitcoin/training_data/ytest_\" +file_postfix+\".dat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# explained variance \n",
    "\n",
    "print 'percent: ', sum([1 for i in var_explain if i>=0.95])*1.0/len(var_explain)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.set_size_inches( 5,5 )\n",
    "ax.hist(var_explain, histtype='bar',  \\\n",
    "        label=['Variance explained'], bins = 30)\n",
    "# ax.set_title('Price proposed by ask and bit requests on one minute', fontsize=13)\n",
    "ax.set_xlabel('Variance', fontsize=13)\n",
    "ax.set_xlim(0.95,1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
