
---- Finished ----

pipeline: feature extraction module, traing-testing data format, training and evalution of models, 

serverl models: gradient boosted tree, random forest, bayesian regression

For neural network: feature normalization 

likelihood evalution

mixture linear

mixture rnn

Batch normalization (for feed-forward and convolutional neural networks) and layer normalization (for recurrent neural networks) 

number of minute distribution 

AIC BIC

skewness

regularization on tree and regression methods, bayesian parameters

fixed expert variance 
include ask bid request features 
absolute errors

outliers in minute features 

Ling-box test on autocorrelation

Studentâ€™s t

Gaussain process, Lasso

Feature seletion
  Feature selection for high-dimensional data: A fast correlation-based filter solution
  Feature Subset Selection and Feature Ranking for Multivariate Time Series

GARCH, EGARCH

Feature engineering
  bid-ask spread
  quantile 
  order imbalance
  depth chart that shows the cumulative bids and asks
  price shift
  slope
  
Structural time series

bilinear
  L1/L0 norm

bilinear in log norm

lasso bilinear

loss function

MAPE error 
---- To Do ----

reload all data and record month location 

https://aksarkar.github.io/nwas/example.html

result analysis: visualize each expert prediction 

Seed the random number generator



relation between mixture weigth and volatility up/down
 
pattern 

confidence interval 

Regularization:
  Automatic Relevance Determination
  l1,2
  group lasso, graphical lasso
  
 
Evaluation metric:
   Average percenate error
   
   Likelihood:
    GP
    Regression

    ARIMA
    Structural

    Mixture
    

Evaluation process:
   single volatility
   volatility + order book
   volatility + feature selection from order book 
   
   mixture model 

Training scheme:
  Rolling and incremental training / testing 

Mixture:
  
  hierachical
   
  Auxililary loss: diversity of experts:  Learning Factored Representations in a Deep Mixture of Experts
  Sparse & regularization:
  Relevance Vector Machines
  Spike and slab 
  

Baseline:
  moving average and filter
  aggregate to the same grnularity, gaussian process 
  



Bayesian:
  mixture regression 
  temporal mixture regression 
  
Gaussain process volatility model 

Feature engineering:
  quantile features
  Seasonality check 
  Bitcoin - CNY
  QQ plot
  correlation matrix
  
Model variance reduction:
  https://stackoverflow.com/questions/45206910/tensorflow-exponential-moving-average
  https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage

Neural Mixture: 
   attention
   gate constraint, gate stability problem 
   gate logit activaition from neural network 
   smooth on expert gate: Learning Factored Representations in a Deep Mixture of Experts
   activation on the last layer
   bias in _linear function

test first, exponetial regression, log normal regression

variance reduction in the estimator for sampling 


---- python math operators ----

np.subtract.outer
squeeze
np.linalg.inv(C).dot(B.T).T.dot(y)

np.outer(b, a)
np.add( [tmp1, tmp2] )

---- Paper write-up ----

Features of volatility series:  no unit root, stocastic trend, local level shift 


attention, mixture, hierachical structure, heterogenous 

Improved expressiveness, Improved generalization

mixture of feature specific expert


One way traders can view order book depth, in addition to the method above, is to use a depth chart that shows the cumulative bids and asks in the current market. This technique illustrates the total volume on the order books starting from the value of the latest transaction.


---- Reference ----

https://tradeblock.com/blog/bitcoin-trading-interpreting-order-books/


-- Deep learning and attention

multi-head attention:  all you need is attention


Outrageously large neural networks the sparsely gated mixture of expert layer

Attention is all you need

Multiple object recognition with visual attention 

The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process : Monte Carlo Gradient

Residual RNN

(2017)hierarchical multiscale RNN

Distilling the Knowledge in a Neural Network

outrageously large neural networks the sparsely gated mixture of expert layer

Fine-Grained Classification via Mixture of Deep Convolutional Neural Networks

Learning Factored Representations in a Deep Mixture of Experts

Adaptive Feature Abstraction for Translating Video to Text

-- Statistical

temporal features 

Sparse Bilinear Logistic Regression


bayesian mixture models for multivariate time series with an application to austrilian rainfall data

paper Inferring The Latent Structure of Human Decision-Making from Raw

4.3. Variance Reduction Policy gradients methods are notorious for suffering from high-variance gradients, since it is computationally expensive to obtain enough rollouts from the simulator to match the policy distribution. Therefore, we apply several variance-reduction techniques, such as replay buffer (Schaul et al., 2015) and baseline methods (Williams, 1992).

ADVI: variational inference

(2015)Probabilistic Dynamic Causal Model for Temporal

A user-centric model of voting intention from Social Media

-- time series

(2015)unsupervised feature learning from temporal data

Extracting Interpretable Features for Early Classification on Time Series

Structure-based Statistical Features and Multivariate Time Series Clustering


-- bitcoin

Trading Bitcoin and Online Time Series Prediction

Quantifying bid-ask spreads in the Chinese stock market using
limit-order book data

Order Book Characteristics and the Volume-Volatility Relation: Empirical Evidence from a Limit Order Market


---- Evaluation ----

hyper-parameters for the benchmark models were chosen

the sensitivity of the model's performance with respect to the architecture of the 'Significance' network

rmse, 

prediction distribution  A v.s. B 

log-likelihood 

Performance w.r.t. different data property, approach property

relation reveald by the model

Wilcoxon signed-rank test



