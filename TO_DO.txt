
---- Finished ----

pipeline: feature extraction module, traing-testing data format, training and evalution of models, 

serverl models: gradient boosted tree, random forest, bayesian regression

For neural network: feature normalization 

likelihood evalution

mixture linear

mixture rnn

Batch normalization (for feed-forward and convolutional neural networks) and layer normalization (for recurrent neural networks) 

number of minute distribution 

AIC BIC

skewness

regularization on tree and regression methods, bayesian parameters

fixed expert variance 
include ask bid request features 
absolute errors

outliers in minute features 

Ling-box test on autocorrelation

Student’s t

Gaussain process, Lasso

Feature seletion
  Feature selection for high-dimensional data: A fast correlation-based filter solution
  Feature Subset Selection and Feature Ranking for Multivariate Time Series

GARCH, EGARCH

Feature engineering
  bid-ask spread
  quantile 
  order imbalance
  depth chart that shows the cumulative bids and asks
  price shift
  slope
  
Structural time series

bilinear
  L1/L0 norm

bilinear in log norm

lasso bilinear

loss function

MAPE error 

result analysis: visualize each expert prediction 

Seed the random number generator

Training scheme:
  Rolling and incremental training / testing 

test first, exponetial regression, log normal regression

# seperate training and validaiton training 

relation between mixture weigth and volatility up/down
 
pattern 

Evaluation process:
   single volatility
   volatility + order book
   volatility + feature selection from order book 
   
mixture model 

post-effect:  volatility patten after different order book weights 


nonstationariety test on weight sequence

error test 

MAE 

SMAPE was used as a forecast error metric defined as
|ŷ−y t |
100
n ×Σ n | y ˆ t |+|y t | /2.


Rolling evalution on statistis baselines

Paper write-up:
   
   cycle and bias terms in structural time series model,  
   

Baseline:
  moving average and filter
  aggregate to the same grnularity, gaussian process 
  

Bayeisan variational


---- To Do ----

Implementation: 
   
   test constant variance 

   proximal gradient for lasso 
   Bayeisan gibbs

   derivation of mixture variance 

   posterior probability of hidden state



proof of convex of loss function 

Baseline: dynamical linear system, phrophet, ARIMA+GARCH

survey KDD, ICDM

multi-step ahead: order book effect decreases

Neural approach:
   
  relu or linear output activation

Evaluation:

  evaluation on non-negative mean regularization 

  when the difference between mixture and baseline is close, pattern of order book 

  Baselines: prophet, gaussian process, nips temporal regularizer, simple random walk and exponential weighted model
 

  Mutual information for hyper-parameters selection 

  hyper-parameters for the benchmark models were chosen

  the sensitivity of the model's performance with respect to the architecture of the 'Significance' network


  prediction distribution  A v.s. B 

  log-likelihood 

  Performance w.r.t. different data property, approach property

  relation reveald by the model

  Wilcoxon signed-rank test


Experiments steps:
  
  two month data 
    
    - only volatility model
    - regression on volatility + features 
    - regression on volatility + feature selection
    - regression on mixture
    
    + result analysis pipeline
    
  incremental data
  
  performance w.r.t. lag order of hourly volatility and minute features, feature selection dim, backward interval 
  
  
reload all data and record month location 

https://aksarkar.github.io/nwas/example.html

confidence interval 


Regularization:
  Automatic Relevance Determination
  l1,2
  group lasso, graphical lasso
  
 
Evaluation metric:
   Average percenate error
   
   Likelihood:
    GP
    Regression

    ARIMA
    Structural

    Mixture
    

Mixture:
  
  hierachical
   
  Auxililary loss: diversity of experts:  Learning Factored Representations in a Deep Mixture of Experts
  Sparse & regularization:
  Relevance Vector Machines
  Spike and slab 
  

Bayesian:
  mixture regression 
  temporal mixture regression 
  
Gaussain process volatility model 

Feature engineering:
  quantile features
  Seasonality check 
  Bitcoin - CNY
  QQ plot
  correlation matrix
  
Model variance reduction:
  https://stackoverflow.com/questions/45206910/tensorflow-exponential-moving-average
  https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage

Neural Mixture: 
   attention
   gate constraint, gate stability problem 
   gate logit activaition from neural network 
   smooth on expert gate: Learning Factored Representations in a Deep Mixture of Experts
   activation on the last layer
   bias in _linear function



variance reduction in the estimator for sampling 


---- python math operators ----

np.subtract.outer
squeeze
np.linalg.inv(C).dot(B.T).T.dot(y)

np.outer(b, a)
np.add( [tmp1, tmp2] )

---- Paper write-up ----

Features of volatility series:  no unit root, stocastic trend, local level shift 


attention, mixture, hierachical structure, heterogenous 

Improved expressiveness, Improved generalization

mixture of feature specific expert


One way traders can view order book depth, in addition to the method above, is to use a depth chart that shows the cumulative bids and asks in the current market. This technique illustrates the total volume on the order books starting from the value of the latest transaction.


---- Reference ----

https://tradeblock.com/blog/bitcoin-trading-interpreting-order-books/


-- Deep learning and attention

A Neural Stochastic Volatility Model

multi-head attention:  all you need is attention

Outrageously large neural networks the sparsely gated mixture of expert layer

Attention is all you need

Multiple object recognition with visual attention 

The Neural Hawkes Process: A Neurally Self-Modulating Multivariate Point Process : Monte Carlo Gradient

Residual RNN

(2017)hierarchical multiscale RNN

Distilling the Knowledge in a Neural Network

outrageously large neural networks the sparsely gated mixture of expert layer

Fine-Grained Classification via Mixture of Deep Convolutional Neural Networks

Learning Factored Representations in a Deep Mixture of Experts

Adaptive Feature Abstraction for Translating Video to Text







