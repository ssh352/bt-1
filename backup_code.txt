
# ---- Mixture linear LOG-NORMAL likelihood ----

# constant prediction problem
class mixture_linear_lognorm_lk():
    
    def __init__(self, session, lr, l2, batch_size, order_v, order_distr, order_steps, bool_bilinear):
        
        # build the network graph 
        self.LEARNING_RATE = lr
                
        self.N_BATCH = batch_size
        self.L2 = l2
   
        self.sess = session
        
        # initialize placeholders
        self.v_auto = tf.placeholder(tf.float32, [None, order_v])
        self.y     = tf.placeholder(tf.float32, [None, ])
        
        if bool_bilinear == True:
            self.distr = tf.placeholder(tf.float32, [None, order_steps, order_distr])
        else:
            self.distr = tf.placeholder(tf.float32, [None, order_distr])
        tmp_d = tf.reshape(self.distr, [-1, order_steps*order_distr])
        
        self.keep_prob = tf.placeholder(tf.float32)
        
        # -- prediction of individual models 
        
        pre_v, regu_v_pre = linear_predict(self.v_auto, order_v, 'pre_v', True )
        
        if bool_bilinear == True:
            
            pre_distr, regu_d_pre = bilinear(self.distr, [order_steps, order_distr], 'pre_distr', True)
        
        else:
            pre_distr, regu_d_pre = linear_predict(self.distr, order_distr, 'pre_distr', True)
        
        # concatenate individual means 
        #pre_stack = tf.stack( [pre_v, pre_distr], 1 )
        
        
        
        # --- variance of individual models 
        
        varv, regu_v_var = linear(self.v_auto, order_v, 'sig_v', True)
        
        if bool_bilinear == True:
            
            vardistr, regu_d_var = linear( tmp_d, order_steps*order_distr, 'sig_distr', True )
            
        else:
            vardistr, regu_d_var = linear( self.distr, order_distr, 'sig_distr', True)

        # variance standardize 
        var_v = tf.square(varv)
        var_d = tf.square(vardistr)
        
        var_stack = tf.stack( [var_v, var_d], 1 )
        
        # --- gate of individual models 
        
        '''
        if bool_bilinear == True:
            
            self.logit, regu_gate = bilinear_with_external( self.distr, [order_steps, order_distr], \
                                                               self.v_auto, order_v, 'gate_distr', True )
            
            #bilinear(self.distr, [order_steps, order_distr], 'gate_distr', True)
            
        else:
            # ?
            logit, regu_gate = linear(self.distr, order_distr, 'gate_distr', True)
        
        
        self.gates = tf.stack( [tf.sigmoid(self.logit), 1.0 - tf.sigmoid(self.logit)] ,1 )
        '''
        
        '''
        vec_gate_d = tf.square( tf.expand_dims(pre_distr, -1) - tf.expand_dims(pre_v,-1) )
        vec_gate_v = tf.concat( [vec_gate_d, self.v_auto], 1)
        
        logit_v, regu_v_gate = linear(vec_gate_v, order_v+1, 'gate_v', True)
        
        if bool_bilinear == True:
            
            logit_distr, regu_d_gate = bilinear_with_external( self.distr, [order_steps, order_distr], \
                                                              vec_gate_d, 1, 'gate_distr', True )
            
            #bilinear(self.distr, [order_steps, order_distr], 'gate_distr', True)
            
        else:
            logit_distr, regu_d_gate = linear(self.distr, order_distr, 'gate_distr', True)
        
        self.logit = tf.squeeze( tf.stack( [logit_v, logit_distr], 1 ) )
        self.gates = tf.nn.softmax(self.logit)
        '''
        
        # based on multimodal distribution        
        logit_v, regu_v_gate = linear(self.v_auto, order_v, 'gate_v', True)
        
        if bool_bilinear == True:
            
            logit_distr, regu_d_gate = bilinear(self.distr, [order_steps, order_distr], 'gate_distr', True)
            
        else:
            logit_distr, regu_d_gate = linear(self.distr, order_distr, 'gate_distr', True)
        
        self.logit = tf.squeeze( tf.stack( [logit_v, logit_distr], 1 ) )
        self.gates = tf.nn.softmax(self.logit)
        
        
        
        # --- negative log likelihood of log normal 
         
        # ? 
        #tmpllk_v = tf.exp(-0.5*tf.square(tf.log(self.y+1e-10) - pre_v)/(var_v+1e-10))/(2.0*np.pi*(var_v+1e-10))**0.5/(self.y+1e-10)
        #tmpllk_distr = tf.exp(-0.5*tf.square(tf.log(self.y+1e-10) - pre_distr)/(var_d+1e-10))/(2.0*np.pi*(var_d+1e-10))**0.5/(self.y+1e-10)
        
        tmpllk_v = tf.exp(-0.5*tf.square(self.y - pre_v)/(var_v+1e-5))/(2.0*np.pi*(var_v+1e-5))**0.5
        tmpllk_distr = tf.exp(-0.5*tf.square(tf.log(self.y+1e-5)-pre_distr)/(var_d+1e-5))/(2.0*np.pi*(var_d+1e-5))**0.5/(self.y+1e-5)
        
        llk = tf.multiply((tf.stack([tmpllk_v, tmpllk_distr], 1)), self.gates ) 
        self.neg_logllk = tf.reduce_sum(-1.0*tf.log( tf.reduce_sum(llk, 1)+1e-10 ))
        
        # --- mixture prediction and errors
        
        # ? var_stack
        #self.y_hat = tf.reduce_sum(tf.multiply(tf.exp(pre_stack), self.gates), 1)
        
        self.orig_pre_v = pre_v
        self.pre_v = pre_v
        #self.pre_v = tf.exp(pre_v)
        self.pre_d = tf.exp(pre_distr)
        
        #?
        #self.pre_v = tf.exp(pre_v - var_v)
        #self.pre_d = tf.exp(pre_distr)
        
        self.pre_stack = tf.stack([self.pre_v, self.pre_d], 1)
        
        self.y_hat = tf.reduce_sum(tf.multiply(self.pre_stack, self.gates), 1)
        
        # prediction errors
        self.err = tf.losses.mean_squared_error( self.y, self.y_hat )
        
        # --- regularization
        
        # mixture diversity 
        
        tmp_gate = tf.transpose(self.gates, [1,0])
        #regu_diver = tf.reduce_mean( tf.square(pre_distr - pre_v) )
        #regu_diver = (tf.square(pre_distr - tf.reduce_mean(pre_distr)) + tf.nn.l2_loss(pre_v))/batch_size
        
        # smoothness of mixture gates
        
        #regu_smooth = tf.square(tmp_gate[0, 1:] - tmp_gate[0,:-1])/batch_size + \
        #              tf.square(tmp_gate[1, 1:] - tmp_gate[1,:-1])/batch_size
        
        tmp_logits = tf.transpose(self.logit, [1,0])
        regu_smooth = tf.square(tmp_logits[0, 1:] - tmp_logits[0,:-1])/batch_size + \
                      tf.square(tmp_logits[1, 1:] - tmp_logits[1,:-1])/batch_size
        
        
        tmpmean = tf.reduce_mean(self.logit)
        regu_gate_diver = tf.reduce_mean(tf.square(self.logit - tmpmean))
        
        # test
        self.test = tf.shape(regu_gate_diver)
        
        
        # parameter regularization 
        if bool_bilinear == True:
            
            # group of l2 + group of l1
            
            self.regu = 0.01*(regu_v_pre) + 0.001*(regu_d_pre[0]) + 0.01*(regu_v_gate + regu_d_gate[0]) + \
            0.01*(regu_v_var + regu_d_var) + 0.001*(regu_d_pre[1]) + 0.01*(regu_d_gate[1])
            
            # for l2
            #self.regu = 0.01*(regu_v_pre + regu_d_pre[0]) + 0.0001*(regu_v_gate + regu_d_gate[0]) + \
            #0.00001*(regu_v_var + regu_d_var) + 0.01*(regu_d_pre[1]) + 0.0001*(regu_d_gate[1])
            
            # for l1
            #self.regu = 0.01*(regu_v_pre + regu_d_pre[0]) + 0.01*(regu_v_gate + regu_d_gate[0]) + \
            #0.00001*(regu_v_var + regu_d_var) + 0.1*(regu_d_pre[1]) + 0.1*(regu_d_gate[1])
            
            # for proximal gradient descent
            #self.regu = (regu_v_pre + regu_d_pre[0]) + (regu_v_gate + regu_d_gate[0]) + \
            #(regu_v_var + regu_d_var) + (regu_d_pre[1]) + (regu_d_gate[1])
        
        else:
            self.regu = 0.01*(regu_v_pre + regu_d_pre) + 0.0001*(regu_v_gate + regu_v_gate) + 0.0001*(regu_v_var + regu_d_var)
    
    
    def model_reset(self):
        self.init = tf.global_variables_initializer()
        self.sess.run( self.init )
        
#   initialize loss and optimization operations for training
    def train_ini(self):
        
        # loss: mixed likelihood + regularization 
        self.lk_loss = self.neg_logllk + self.regu
        
        # ? 
        self.optimizer = tf.train.AdamOptimizer(learning_rate = self.LEARNING_RATE).minimize(self.lk_loss)
        #self.LEARNING_RATE
        # !
        #self.optimizer = tf.train.ProximalAdagradOptimizer(learning_rate = 0.1, \
        #                                                  l2_regularization_strength = 0.0001, \
        #                                                  l1_regularization_strength = 0.01).minimize(self.lk_loss)
        
        self.init = tf.global_variables_initializer()
        self.sess.run( self.init )
    
    
    #   training on batch of data
    def test_batch(self, v_train, distr_train, y_train, keep_prob ):
        
        # !
        c = self.sess.run( self.test,\
                             feed_dict={self.v_auto:v_train,\
                                        self.distr:distr_train, self.y:y_train, self.keep_prob:keep_prob })
        return c
    
    #   training on batch of data
    def train_batch(self, v_train, distr_train, y_train, keep_prob ):
        
        # !
        _, c = self.sess.run([self.optimizer, self.lk_loss],\
                             feed_dict={self.v_auto:v_train,\
                                        self.distr:distr_train, self.y:y_train, self.keep_prob:keep_prob })
        return c
    
    
    #   infer givn testing data
    def inference(self, v_test, distr_test, y_test, keep_prob):
        
        return self.sess.run([self.err, self.regu], feed_dict = {self.v_auto:v_test, \
                                                      self.distr:distr_test,  self.y:y_test, self.keep_prob:keep_prob })
    
    #   predict givn testing data
    def predict(self, v_test, distr_test, keep_prob):
        
        return self.sess.run( [self.y_hat, self.pre_v, self.pre_d, self.orig_pre_v], feed_dict = {self.v_auto:v_test,  \
                                                       self.distr:distr_test,  self.keep_prob:keep_prob })
    
     #   predict givn testing data
    def predict_gates(self, v_test, distr_test, keep_prob):
        return self.sess.run( self.gates , feed_dict = {self.v_auto:v_test,   \
                                                        self.distr:distr_test,  self.keep_prob:keep_prob })
    
    def predict_logit(self, v_test, distr_test, keep_prob):
        return self.sess.run( self.logit , feed_dict = {self.v_auto:v_test,   \
                                                        self.distr:distr_test,  self.keep_prob:keep_prob })
    
    def collect_coeff_values(self, vari_keyword):
        return [ tf_var.name for tf_var in tf.trainable_variables() if (vari_keyword in tf_var.name) ],\
    [ tf_var.eval() for tf_var in tf.trainable_variables() if (vari_keyword in tf_var.name) ]
    
    
 
