{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# periodic time series\n",
    "\n",
    "#  time series modeling process\n",
    "\n",
    "# 1. stationary test\n",
    "#    trend model:\n",
    "#         Aggregation – taking average for a time period like monthly/weekly averages\n",
    "#         Smoothing – taking rolling averages\n",
    "#         Polynomial Fitting – fit a regression model\n",
    "#    seasonality model:\n",
    "#         differencing\n",
    "#         model fitting\n",
    "# 2. stationalize time series\n",
    "#       detrend\n",
    "#       remove seasonality\n",
    "# 3. model the stationary part of time series        \n",
    "\n",
    "#    seasonal-ARIMA \n",
    "\n",
    "# 4. perform prediction \n",
    "\n",
    "\n",
    "# TO DO\n",
    "# plain time series prediction\n",
    "# periodic time series prediction\n",
    "\n",
    "# https://dandelion.eu/datamine/open-big-data/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://www.johnwittenauer.net/a-simple-time-series-analysis-of-the-sp-500-index/\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TO DO: \n",
    "\n",
    "# Statistics programming \n",
    "\n",
    "# book:\n",
    "\n",
    "# Probabilistic Programming and Bayesian Methods for Hackers \n",
    "# http://nbviewer.jupyter.org/github/CamDavidsonPilon/Probabilistic-Programming-and-Bayesian-Methods-for-Hackers/blob/master/Prologue/Prologue.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Examples:\n",
    "\n",
    "# stock prediction\n",
    "# https://bitbucket.org/joexdobs/ml-classifier-gesture-recognition/wiki/stock-example/\n",
    "# predict-future-stock-price-using-machine-learning.md\n",
    "\n",
    "# statsmodels:\n",
    "#  http://statsmodels.sourceforge.net/stable/index.html\n",
    "\n",
    "# http://www.seanabu.com/2016/03/22/time-series-seasonal-ARIMA-model-in-python/\n",
    "\n",
    "# http://nbviewer.jupyter.org/gist/ChadFulton/5127108f4c7025ed2648\n",
    "\n",
    "# https://www.analyticsvidhya.com/blog/2016/02/time-series-forecasting-codes-python/\n",
    "\n",
    "# http://nbviewer.jupyter.org/github/jakevdp/SeattleBike/blob/master/SeattleCycling.ipynb\n",
    "\n",
    "# book\n",
    "# http://shelfjoy.com/shelfjoy/17-essential-machine-learning-books-suggested-by-michael-i-jordan-from-berkeley"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anino/miniconda2/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline    \n",
    "import matplotlib as mplt\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.ticker import FuncFormatter\n",
    "import matplotlib.pyplot as plt\n",
    "# matplotlib.style.use('ggplot')\n",
    "\n",
    "from utils_libs import *\n",
    "from utils_data_prep import *\n",
    "\n",
    "from scipy.stats import lognorm\n",
    "from scipy.stats import norm\n",
    "from scipy.stats import chisqprob\n",
    "\n",
    "from numpy import prod\n",
    "import seaborn as sns\n",
    "\n",
    "# statiscal models\n",
    "import statsmodels as sm\n",
    "from statsmodels.tsa.stattools import acf  \n",
    "from statsmodels.tsa.stattools import pacf\n",
    "from statsmodels.tsa.api import VAR, DynamicVAR\n",
    "\n",
    "from statsmodels.stats import diagnostic\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "539157 10641\n"
     ]
    }
   ],
   "source": [
    "# --- Load pre-processed order book data ---\n",
    "\n",
    "all_dta_minu = np.load(\"../dataset/bitcoin/dta_minu.dat\")\n",
    "all_loc_hour = np.load(\"../dataset/bitcoin/loc_hour.dat\")\n",
    "print len(all_dta_minu), len(all_loc_hour)\n",
    "\n",
    "# --- Load order book data files ---\n",
    "\n",
    "# all_dta_minu,all_loc_hour = load_raw_order_book_files('../dataset/bitcoin/order_book/*.csv', True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(539157,) (539157, 2)\n",
      "539157 528516 10641 10641\n"
     ]
    }
   ],
   "source": [
    "# --- calculate price, return and volatility ---\n",
    "\n",
    "price_minu, req_minu = cal_price_req_minu(all_dta_minu)\n",
    "\n",
    "print np.shape(price_minu), np.shape(req_minu)\n",
    "\n",
    "pvol_hour = cal_price_volatility_hour( all_loc_hour, price_minu )\n",
    "return_minu, rvol_hour = cal_return_volatility_hour( all_loc_hour, price_minu, 'per' )\n",
    "\n",
    "print len(price_minu),len(return_minu), len(pvol_hour), len(rvol_hour)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --- distributional features ---\n",
    "\n",
    "# analytical posterior: sampling by enumerating and calculating density\n",
    "# approximate posterior: sampling via MCMC\n",
    "    \n",
    "# pymc?\n",
    "def poterior_sample_norm_2d(x, n_samples):\n",
    "    return 1\n",
    "    \n",
    "def poterior_sample_log_norm_2d(x, n_samples):\n",
    "    return 1\n",
    "    \n",
    "def map_log_norm_2d(x):\n",
    "    \n",
    "    if len(x) == 0:\n",
    "        return [0.0, 0.0], [0.0, 0.0, 0.0]\n",
    "    \n",
    "    elif len(x) == 1:\n",
    "        return [ x[0][0], x[0][1] ], [0.0, 0.0, 0.0]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        tmpx = [ [i[0]+1e-5, i[1]+1e-5] for i in x]\n",
    "        logx = log(tmpx)\n",
    "        \n",
    "        post_log_mu, post_log_cov = map_norm_2d(logx)\n",
    "        \n",
    "        post_mu = [ exp(post_log_mu[i] + post_log_cov[i]/2.0) for i in range(2) ]\n",
    "        \n",
    "#         post_cov = [ [0.0, 0.0] for i in range(2) ]\n",
    "        \n",
    "#         for i in range(2):\n",
    "#             for j in range(2):\n",
    "#                 tmp = post_log_cov[2] if i!=j else post_log_cov[i]  \n",
    "#                 post_cov[i][j] = exp( post_log_mu[i]+post_log_mu[j]+0.5*(post_log_cov[i]+post_log_cov[j]) )*\\\n",
    "#                 ( exp(tmp)-1.0 )\n",
    "                \n",
    "        var0 = exp( post_log_mu[0]+post_log_mu[0]+0.5*(post_log_cov[0]+post_log_cov[0]) )*\\\n",
    "        ( exp(post_log_cov[0])-1.0 )\n",
    "        var1 = exp( post_log_mu[1]+post_log_mu[1]+0.5*(post_log_cov[1]+post_log_cov[1]) )*\\\n",
    "        ( exp(post_log_cov[1])-1.0 )\n",
    "        cov = exp( post_log_mu[0]+post_log_mu[1]+0.5*(post_log_cov[0]+post_log_cov[1]) )*\\\n",
    "        ( exp(post_log_cov[2])-1.0 )\n",
    "        \n",
    "        return list(post_mu), [var0, var1, cov], list(post_mu), \\\n",
    "               [post_log_cov[0][0], post_log_cov[1][1], post_log_cov[0][1]] \n",
    "\n",
    "def map_norm_2d( x ):\n",
    "    \n",
    "    if len(x) == 0:\n",
    "        return [0.0, 0.0], [0.0, 0.0, 0.0]\n",
    "    \n",
    "    elif len(x) == 1:\n",
    "        return [ x[0][0], x[0][1] ], [0.0, 0.0, 0.0]\n",
    "    \n",
    "    else:\n",
    "        mle_mu  = np.mean(x, axis=0) \n",
    "        mle_cov = np.cov(x, rowvar=0)\n",
    "    \n",
    "        m_0 = mle_mu\n",
    "        k0  = 0.01\n",
    "        v0 = 2.0 + 2.0\n",
    "        S_0 = np.diag(np.diag(mle_cov))*1.0/len(x)\n",
    "    \n",
    "        x_ba = mle_mu\n",
    "    \n",
    "        #S = np.zeros((2, 2))\n",
    "        #for i in x:\n",
    "        #    S = np.add(S, np.outer(i, i))\n",
    "            \n",
    "        S = np.matmul( np.asmatrix(x).transpose() , np.asmatrix(x) )\n",
    "        \n",
    "        N = len(x)\n",
    "        m_N = k0*1.0/(k0+N)*m_0 + N*1.0/(k0+N)*x_ba \n",
    "    \n",
    "        vN = v0 + N    \n",
    "        kN = k0 + N\n",
    "        \n",
    "        S_N = S_0 + S + k0*np.outer(m_0, m_0) - kN*np.outer(m_N, m_N)\n",
    "        \n",
    "        cov_mode = S_N*1.0/(vN+2.0+2.0)\n",
    "        \n",
    "        return list(m_N), [ cov_mode.item((0, 0)), cov_mode.item((1, 1)), cov_mode.item((1, 0)) ] \n",
    "    \n",
    "def mle_norm_2d( x ):\n",
    "    \n",
    "    if len(x) == 0:\n",
    "        return [0.0, 0.0], [0.0, 0.0]\n",
    "    elif len(x) == 1:\n",
    "        return [x[0][0], x[0][1]], [0.0, 0.0]\n",
    "    else:\n",
    "        tmp = np.cov(x, rowvar=0)\n",
    "        return list(np.mean(x, axis=0)), [tmp[0][0], tmp[1][1]] \n",
    "    \n",
    "def skewness(x):\n",
    "    \n",
    "    if len(x) == 0:\n",
    "        return [0.0, 0.0]\n",
    "\n",
    "    elif len(x) == 1:\n",
    "        return [0.0, 0.0]\n",
    "    \n",
    "    else:\n",
    "        return list(sp.stats.skew(x,0))\n",
    "    \n",
    "def loglk_norm( x, mu, cov ):\n",
    "    \n",
    "    var = multivariate_normal(mean=mu, cov=[[cov[0], cov[2]], [cov[2],cov[1]]])\n",
    "    return sum( var.logpdf(x) )\n",
    "\n",
    "def likelihood_ratio_test(llmin, llmax, df):\n",
    "    return sp.stats.chisqprob(-2.0*(llmin-llmax), df)\n",
    "\n",
    "\n",
    "def bid_ask_spread(all_dta_minu, tmp_idx):\n",
    "    x_a = all_dta_minu[tmp_idx][0]\n",
    "    x_b = all_dta_minu[tmp_idx][1]\n",
    "    \n",
    "    #if ask side is empty --- find last ask side and use it\n",
    "    if (market_depth_a_volume(x_a)==0):\n",
    "        return abs(find_last_ask_price(all_dta_minu, tmp_idx)-x_b[0][0])\n",
    "    \n",
    "    #if bid side is empty -- find last bid side and use it\n",
    "    if (market_depth_b_volume(x_b)==0):\n",
    "        return abs(x_a[0][0] - find_last_bid_price(all_dta_minu, tmp_idx))\n",
    "    \n",
    "    #calucate difference -- spread\n",
    "    return abs(x_a[0][0]-x_b[0][0])\n",
    "\n",
    "def bid_ask_spread_weighted(all_dta_minu, tmp_idx):\n",
    "    x_a = all_dta_minu[tmp_idx][0]\n",
    "    x_b = all_dta_minu[tmp_idx][1]\n",
    "    \n",
    "    #either bid or ask side is empty -- call just bid_ask_spread function\n",
    "    if ((market_depth_b_volume(x_b)==0)|(market_depth_a_volume(x_a)==0)):\n",
    "        return bid_ask_spread(all_dta_minu, tmp_idx)\n",
    "    \n",
    "    \n",
    "    # calculate avg bid on first 10 % of orders\n",
    "    idx = np.shape(x_b)[0]/10\n",
    "    \n",
    "    if (idx==0): #smaller than 10\n",
    "        idx = np.shape(x_b)[0]\n",
    "    \n",
    "    #cumulative price of 10% of bid volume\n",
    "    cum_bid = 0.0\n",
    "    for i in range ( idx ):\n",
    "        cum_bid+=x_b[i][0]\n",
    "    cum_bid = cum_bid/idx\n",
    "    \n",
    "    # calculate avg ask on first 10 % of orders\n",
    "    idx = np.shape(x_a)[0]/10\n",
    "    \n",
    "    if (idx==0): #smaller than 10\n",
    "        idx = np.shape(x_a)[0]\n",
    "    \n",
    "    #cumulative price of 10% of bid volume\n",
    "    cum_ask = 0.0\n",
    "    for i in range ( idx ):\n",
    "        cum_ask+=x_a[i][0]\n",
    "    cum_ask = cum_ask/idx\n",
    "    \n",
    "    return abs(cum_ask-cum_bid)\n",
    "\n",
    "def market_depth_a_volume(x_a):\n",
    "    #number of orders\n",
    "    return (np.shape(x_a)[0])\n",
    "\n",
    "def market_depth_b_volume(x_b):\n",
    "    #number of orders\n",
    "    return (np.shape(x_b)[0])\n",
    "\n",
    "def market_depth_a_btc(x_a):\n",
    "    #sum of btc in ask side\n",
    "    btc_sum = 0.0\n",
    "    for i in range ( np.shape(x_a)[0] ):\n",
    "        btc_sum+=x_a[i][1]\n",
    "    return btc_sum\n",
    "\n",
    "def market_depth_b_btc(x_b):\n",
    "    #sum of btc in bid side\n",
    "    btc_sum = 0.0\n",
    "    for i in range ( np.shape(x_b)[0] ):\n",
    "        btc_sum+=x_b[i][1]\n",
    "    return btc_sum\n",
    "\n",
    "def find_last_bid_price(all_dta_minu, idx):\n",
    "    #search for last available bid\n",
    "    tmp_b = all_dta_minu[idx][1]\n",
    "    while(market_depth_b_volume(tmp_b)==0):\n",
    "        idx=idx-1;\n",
    "        tmp_b = all_dta_minu[idx][1]\n",
    "        \n",
    "    return tmp_b[0][0]\n",
    "\n",
    "def find_last_ask_price(all_dta_minu, idx):\n",
    "    #search for last available ask\n",
    "    tmp_a = all_dta_minu[idx][0]\n",
    "    while(market_depth_a_volume(tmp_a)==0):\n",
    "        idx=idx-1;\n",
    "        tmp_a = all_dta_minu[idx][0]\n",
    "        \n",
    "    return tmp_a[0][0]\n",
    "\n",
    "def bid_ask_slope(all_dta_minu, tmp_idx):\n",
    "    #calculated the volume in the tail that belongs closest to the current price\n",
    "    #essentially sum until some delta price -- is estimated from data -> from first 10 % of orders\n",
    "    x_a = all_dta_minu[tmp_idx][0]\n",
    "    x_b = all_dta_minu[tmp_idx][1]\n",
    "    \n",
    "    if (market_depth_b_volume(x_b)==0): #bid is empty\n",
    "        cum_bid = 0.0\n",
    "        idx = np.shape(x_a)[0]/10 #use 10% on ask sid\n",
    "        delta = abs(x_a[idx][0] - x_a[0][0]) #critical value for summation\n",
    "    else:\n",
    "        #find delta valule for price for the first 10% of orders on bid side and use it also for ask side\n",
    "        idx = np.shape(x_b)[0]/10\n",
    "        delta = abs(x_b[idx][0] - x_b[0][0]) #critical value for summation\n",
    "    \n",
    "        #cumulative volume of orders on bid side until the delta price\n",
    "        cum_bid = 0.0\n",
    "        for i in range ( idx ):\n",
    "            cum_bid+=x_b[i][1]\n",
    "        \n",
    "    if (market_depth_a_volume(x_a)==0): #ask is empty\n",
    "        cum_ask = 0\n",
    "    else:    \n",
    "        #cumulative volume of orders on ask side until the delta price\n",
    "        cum_ask = 0.0\n",
    "        for i in range ( np.shape(x_a)[0] ):\n",
    "            if ( x_a[i][0]  <= (delta+x_a[0][0]) ):\n",
    "                cum_ask+=x_a[i][1]\n",
    "    \n",
    "    \n",
    "    return cum_bid, cum_ask\n",
    "    \n",
    "\n",
    "def orderbook_stat_features(all_dta_minu, tmp_idx):\n",
    "    tmp_a = all_dta_minu[tmp_idx][0]\n",
    "    tmp_b = all_dta_minu[tmp_idx][1]\n",
    " \n",
    "    f = []\n",
    "    f.append(bid_ask_spread(all_dta_minu, tmp_idx))\n",
    "    f.append(bid_ask_spread_weighted(all_dta_minu, tmp_idx))\n",
    "    f.append(market_depth_a_volume(tmp_a))\n",
    "    f.append(market_depth_b_volume(tmp_b))\n",
    "    f.append(market_depth_a_volume(tmp_a)-market_depth_b_volume(tmp_b))\n",
    "    f.append(market_depth_a_btc(tmp_a))\n",
    "    f.append(market_depth_b_btc(tmp_b))\n",
    "    f.append(market_depth_a_btc(tmp_a)-market_depth_b_btc(tmp_b))\n",
    "    cum_bid, cum_ask = bid_ask_slope(all_dta_minu, tmp_idx)\n",
    "    f.append(cum_bid)\n",
    "    f.append(cum_ask)\n",
    "    return f\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(539157, 12)\n"
     ]
    }
   ],
   "source": [
    "# --- extract features w.r.t. minute ---\n",
    "features_minu = [] \n",
    "# ask mean price, mean amount, var price, var amount, \n",
    "# bid mean price, mean amount, var price, var amount,\n",
    "# ask skew price, skew amount\n",
    "# bid skew price, skew amount, \n",
    "# ask request, bid request \n",
    "\n",
    "for i in range( len(all_dta_minu) ):\n",
    "    tmp_a = all_dta_minu[i][0]\n",
    "    tmp_b = all_dta_minu[i][1]\n",
    "#   shape abov e: # by [price, amount]\n",
    "    \n",
    "#   mle_norm_2d() replaced by Nino func  \n",
    "    tmpft = mle_norm_2d(tmp_a)\n",
    "    tmp = tmpft[0] + tmpft[1]\n",
    "    \n",
    "#   mle_norm_2d() replaced by Nino func  \n",
    "    tmpft = mle_norm_2d(tmp_b)\n",
    "    tmp += tmpft[0]\n",
    "    tmp += tmpft[1]\n",
    "    \n",
    "    #   skewness feature\n",
    "    tmp += skewness(tmp_a)\n",
    "    tmp += skewness(tmp_b)\n",
    "    \n",
    "    # financial-like features -- spread, slope, depth\n",
    "    tmp = orderbook_stat_features(all_dta_minu,i)\n",
    "    \n",
    "    features_minu.append( tmp )\n",
    "    \n",
    "    # amount of requests\n",
    "    tmp += [len(all_dta_minu[i][0]), len(all_dta_minu[i][1])] \n",
    "    \n",
    "#     if len(all_dta_minu[i][0]) ==0 or len(all_dta_minu[i][1])==0:\n",
    "#             print len(all_dta_minu[i][0]), len(all_dta_minu[i][1])\n",
    "            \n",
    "#  # of miniutes by # features\n",
    "features_minu = np.reshape( features_minu, (len(features_minu), -1) )\n",
    "\n",
    "print np.shape( np.asarray(features_minu) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_testing_garch(features_minu, vol_hour, all_loc_hour, \\\n",
    "                                  order_minu, order_hour, train_split_ratio, price_minu):\n",
    "    tmpcnt = len(vol_hour)\n",
    "    tmp_split = int(train_split_ratio*(tmpcnt - order_hour - 1)) + order_hour\n",
    "    \n",
    "    minu_idx = all_loc_hour[tmp_split]\n",
    "    \n",
    "    price_train = price_minu[:minu_idx+1]\n",
    "    price_test  = price_minu[minu_idx+1:]\n",
    "    \n",
    "    return_train = []\n",
    "    for i in range(1, len(price_train)):\n",
    "        return_train.append( (price_train[i]-price_train[i-1])/(price_train[i-1]+1e-5)*100 )\n",
    "        \n",
    "    return_test = []\n",
    "    for i in range(1, len(price_test)):\n",
    "        return_test.append( (price_test[i]-price_test[i-1])/(price_test[i-1]+1e-5)*100 )\n",
    "            \n",
    "    # vol train, return train, vol test, return test\n",
    "    return vol_hour[1:tmp_split+1], return_train, vol_hour[tmp_split+1:], return_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8515,) (419602,) (2125,) (119553,)\n",
      "539155 10640\n"
     ]
    }
   ],
   "source": [
    "# --- obtain training and testing data, garch ---\n",
    "para_order_minu = 10\n",
    "para_order_hour = 16\n",
    "para_train_split_ratio = 0.8\n",
    "file_postfix = \"garch\" \n",
    "#mle_norm, map_norm, map_lognorm \n",
    "\n",
    "vol_train, rt_train, vol_test, rt_test = training_testing_garch(features_minu, rvol_hour, all_loc_hour, \\\n",
    "                                   para_order_minu, para_order_hour, para_train_split_ratio, price_minu)\n",
    "\n",
    "print np.shape(vol_train), np.shape(rt_train), np.shape(vol_test), np.shape(rt_test)\n",
    "print len(rt_train)+len(rt_test), len(vol_train)+len(vol_test)\n",
    "\n",
    "np.asarray(vol_train).dump(\"../dataset/bitcoin/training_data/voltrain_\"+file_postfix+\".dat\")\n",
    "np.asarray(rt_train).dump(\"../dataset/bitcoin/training_data/rttrain_\"  +file_postfix+\".dat\")\n",
    "np.asarray(vol_test).dump(\"../dataset/bitcoin/training_data/voltest_\"  +file_postfix+\".dat\")\n",
    "np.asarray(rt_test).dump(\"../dataset/bitcoin/training_data/rttest_\"    +file_postfix+\".dat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8515,) (8515, 140) (2125,) (2125, 140)\n"
     ]
    }
   ],
   "source": [
    "# --- obtain training and testing data, arima, structural time series ---\n",
    "para_order_minu = 10\n",
    "para_order_hour = 16\n",
    "para_train_split_ratio = 0.8\n",
    "file_postfix = \"stat\" \n",
    "#mle_norm, map_norm, map_lognorm \n",
    "\n",
    "xtrain, extrain, xtest, extest = training_testing_statistic(features_minu, rvol_hour, all_loc_hour, \\\n",
    "                                   para_order_minu, para_order_hour, para_train_split_ratio)\n",
    "\n",
    "print np.shape(xtrain), np.shape(extrain), np.shape(xtest), np.shape(extest)\n",
    "\n",
    "np.asarray(xtrain).dump(\"../dataset/bitcoin/training_data/xtrain_\"+file_postfix+\".dat\")\n",
    "np.asarray(xtest ).dump(\"../dataset/bitcoin/training_data/xtest_\" +file_postfix+\".dat\")\n",
    "np.asarray(extrain).dump(\"../dataset/bitcoin/training_data/extrain_\"+file_postfix+\".dat\")\n",
    "np.asarray(extest ).dump(\"../dataset/bitcoin/training_data/extest_\" +file_postfix+\".dat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0555666200111\n"
     ]
    }
   ],
   "source": [
    "tmpres= np.diff(ytest)\n",
    "print sqrt(mean(tmpres*tmpres)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10625\n",
      "(8500, 156) (8500,) (2125, 156) (2125,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- obtain training and testing data, plain regression ---\n",
    "para_order_minu = 10\n",
    "para_order_hour = 16\n",
    "para_train_split_ratio = 0.8\n",
    "file_postfix = \"norm_v_minu_reg\" \n",
    "#mle_norm, map_norm, map_lognorm \n",
    "\n",
    "# features_minu, req_minu, pvol_hour, all_loc_hour\n",
    "x, y = prepare_feature_target( features_minu, [], rvol_hour, all_loc_hour, \\\n",
    "                                                        para_order_minu, para_order_hour )\n",
    "print len(x)\n",
    "    \n",
    "xtrain, ytrain, xtest, ytest = training_testing_plain_regression(x, y, para_train_split_ratio)    \n",
    "print np.shape(xtrain), np.shape(ytrain), np.shape(xtest), np.shape(ytest)\n",
    "\n",
    "# np.asarray(xtrain).dump(\"../dataset/bitcoin/training_data/xtrain_\"+file_postfix+\".dat\")\n",
    "# np.asarray(xtest ).dump(\"../dataset/bitcoin/training_data/xtest_\" +file_postfix+\".dat\")\n",
    "# np.asarray(ytrain).dump(\"../dataset/bitcoin/training_data/ytrain_\"+file_postfix+\".dat\")\n",
    "# np.asarray(ytest ).dump(\"../dataset/bitcoin/training_data/ytest_\" +file_postfix+\".dat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10625\n",
      "2 (8500,) (2125,)\n"
     ]
    }
   ],
   "source": [
    "# --- obtain training and testing data, mixture: linear, mlp --- \n",
    "para_order_minu = 10\n",
    "para_order_hour = 16\n",
    "para_train_split_ratio = 0.8 \n",
    "file_postfix = \"norm_v_minu_mix\"\n",
    "\n",
    "# features_minu, req_minu, pvol_hour, all_loc_hour\n",
    "x, y = prepare_feature_target( features_minu, [], rvol_hour, all_loc_hour, \\\n",
    "                                                        para_order_minu, para_order_hour )\n",
    "print len(x)\n",
    "\n",
    "xtrain, ytrain, xtest, ytest = training_testing_mixture_mlp(x, y, para_train_split_ratio)\n",
    "\n",
    "print len(xtrain[0]), np.shape(ytrain), np.shape(ytest)\n",
    "\n",
    "np.asarray(xtrain).dump(\"../dataset/bitcoin/training_data/xtrain_\"+file_postfix+\".dat\")\n",
    "np.asarray(xtest ).dump(\"../dataset/bitcoin/training_data/xtest_\" +file_postfix+\".dat\")\n",
    "np.asarray(ytrain).dump(\"../dataset/bitcoin/training_data/ytrain_\"+file_postfix+\".dat\")\n",
    "np.asarray(ytest ).dump(\"../dataset/bitcoin/training_data/ytest_\" +file_postfix+\".dat\")\n",
    "\n",
    "#  done for training-test preparaion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10577\n",
      "2 (8461,) (2116,)\n"
     ]
    }
   ],
   "source": [
    "# --- obtain training and testing data, mixture: rnn --- \n",
    "para_order_minu = 10\n",
    "para_order_hour = 64\n",
    "para_train_split_ratio = 0.8\n",
    "file_postfix = \"neu_norm_v_minu_mix\"\n",
    "\n",
    "# features_minu, req_minu, pvol_hour, all_loc_hour\n",
    "x, y = prepare_feature_target( features_minu, [], pvol_hour, all_loc_hour, \\\n",
    "                                                        para_order_minu, para_order_hour )\n",
    "print len(x)\n",
    "\n",
    "xtrain, ytrain, xtest, ytest = training_testing_mixture_rnn(x, y, para_train_split_ratio)\n",
    "\n",
    "print len(xtrain[0]), np.shape(ytrain), np.shape(ytest)\n",
    "\n",
    "np.asarray(xtrain).dump(\"../dataset/bitcoin/training_data/xtrain_\"+file_postfix+\".dat\")\n",
    "np.asarray(xtest ).dump(\"../dataset/bitcoin/training_data/xtest_\" +file_postfix+\".dat\")\n",
    "np.asarray(ytrain).dump(\"../dataset/bitcoin/training_data/ytrain_\"+file_postfix+\".dat\")\n",
    "np.asarray(ytest ).dump(\"../dataset/bitcoin/training_data/ytest_\" +file_postfix+\".dat\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# --- likelihood evaluation ---\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "n_sample = 100\n",
    "tmpidx = range(len(all_dta_minu))\n",
    "np.random.shuffle(tmpidx)\n",
    "sample_dta_minu = all_dta_minu[tmpidx[:n_sample]]\n",
    "\n",
    "for i in sample_dta_minu:\n",
    "    tmpa = i[0]\n",
    "    tmpb = i[1]\n",
    "    \n",
    "    tmp_para  = map_norm_2d(tmpa)\n",
    "    loglk_min = loglk_norm( tmpa, tmp_para[0], tmp_para[1] )\n",
    "    print tmp_para[1][2]\n",
    "#     tmp_rv = sp.stats.norm( tmp_para[0][0], tmp_para[1][0])\n",
    "#     print sp.stats.kstest( [i[0] for i in tmpa] ,tmp_rv.cdf )\n",
    "    \n",
    "#     tmp_rv = sp.stats.norm( tmp_para[0][1], tmp_para[1][1])\n",
    "#     print sp.stats.kstest( [i[1] for i in tmpa] ,tmp_rv.cdf )\n",
    "\n",
    "#     tmp_para = map_norm_2d(tmpa)\n",
    "#     print sum(loglk_norm( tmpa, tmp_para[0], tmp_para[1] ))\n",
    "    \n",
    "    tmplog = [[log(i[0]+1e-07), log(i[1]+1e-07)] for i in tmpa]\n",
    "    tmp_para  = mle_norm_2d( tmplog )\n",
    "    print tmp_para[1][2]\n",
    "    \n",
    "#     loglk_max = loglk_norm( tmplog, tmp_para[0], tmp_para[1] )\n",
    "    \n",
    "#     tmp_rv = sp.stats.norm( tmp_para[0][0], tmp_para[1][0] )\n",
    "#     print sp.stats.kstest( [i[0] for i in tmplog] ,tmp_rv.cdf )\n",
    "    \n",
    "#     tmp_rv = sp.stats.lognorm( tmp_para[0][1], tmp_para[1][1] )\n",
    "#     print sp.stats.kstest( [i[1] for i in tmplog] ,tmp_rv.cdf )\n",
    "\n",
    "#     print loglk_max,loglk_min, likelihood_ratio_test(loglk_min, loglk_max, 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
